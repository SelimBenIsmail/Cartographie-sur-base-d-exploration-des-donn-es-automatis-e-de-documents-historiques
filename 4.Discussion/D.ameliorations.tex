\section{Pistes d'améliorations}
Le mémoire a décrit le développement d'un système traduisant un document historique textuel en cartographie. Le processus par lequel passe le document est long  et traverse nombreux champs des technologie de l'information et des humanités numériques. Dans le cadre de ce  mémoire, qui est avant tout un prototype, un \textit{proof of concept}, ces aspects n'ont pu être explorés que dans une profondeur relative, voire de manière très superficielle pour certain. De fait, les pistes d'amélioration du système sont très nombreuses.



\subsection{Qualité des cartographies}
L'un des premier aspect qui nécessiterait d'être amélioré est la qualité des cartographies produite. L'outil que nous avons utilisé, \textit{ggmap}, à l'avantage d'être une extension de \textit{ggplot2} et d'offrir une grammaire graphique similaire en plus d'être facilement pris en main. Malheureusement, nous sommes rapidement bloqués par les limite de la bibliothèque qui ne permet pas de respecter les règles de bases de la représentation cartographique\footnote{Il est par exemple impossible d'afficher l'orientation du Nord ou l'échelle des distances.}. 
Les bibliothèques \textit{cartography} et \textit{lieflet} semblent être des alternatives, certes, plus complexes à maîtriser, mais apportant une solution plus complète pour la production de cartographies plus élaborées.
En outre, \textit{leaflet} permet la création de cartes interactives qui pourrait s'avérer intéressantes au vu de la quantité de données à afficher.


\subsection{Pondération des arêtes}
Un aspect des graphes qui n'a pas été étudié au cours de ce mémoire est la pondération des arêtes. Pourtant celle-ci pourrait résoudre le problème engendré par les individus débiteurs de plusieurs rentes et qui doivent, par conséquent,  être modéliser en différents sommets. Nous savons que les rentes sont classées dans le registre en suivant une continuité topographique. Une affirmation qui peut être observé également sur les cartes  produites. 
Donc en affectant le numéro de rente successive comme valeur aux arêtes, puis en affectant aux sommets, la valeur moyenne des arêtes qui lui sont incidentes, les arêtes erronés devraient pouvoir être identifiées par la taille de l'écart entre leur poids et celui des sommets auxquels elles sont connectées. Ceci n'est évidemment qu'un exemple théorique de l'utilisation qu'il pourrait être de faire de la pondération des arêtes du graphe.

\subsection{Regroupement des anthroponymes}
Le regroupement d'anthroponymes est une problématique complexe qui ne trouve actuellement aucune solution générale parfaite dans la littérature, il en demeure   pourtant un aspect primordiale dans ce type de d'analyses. Il est par conséquent nécessaire d'apporter des modifications à ces solutions imparfaites pour les rendre plus adéquates aux spécificités du contexte étudié.

Une première approche pour améliorer la reconnaissance et regroupement d'anthroponymes serait de coupler avec d'autres sources ou base de données et d'envisager une stratégie orientée connaissance pour créer un  dictionnaire de différentes graphies.

Une autre approche est d'améliorer l'algorithme de calcul des distances en modulant le poids des opérations lors de cas précis (diminuer le poids de l'ajout ou de la suppression d'un espace pour alléger le score des particules dans les patronymes, par exemple). Cela nécessiterait d'effectuer de nombreux essais conséquents en terme de puissance de calcul\footnote{Pour déterminer les distances séparant chacune des 799 formes d'anthroponymes entre-elles, l'ordinateur a calculé pendant presque une heure}, mais des alternatives existent pour améliorer la vitesse d'exécution de ce type de calcul et elles sont de plus en plus accessibles via le \textit{cloud computing}. On peut citer à titre d'exemple, la parallélisation du code, ou dans un avenir plus lointain, peut-être, l'informatique quantique, lorsque cette technologie aura gagnée en précision et en capacité de mémoire.

\subsection{Gestion de l'information}
%système de table de données
La gestion des données est un aspect important de tout systèmes d'informations.
Dans le système développé, elle consiste principalement en ce qu'on pourrait rapprocher d'une consolidation des données\footnote{<< La consolidation des données est le processus par lequel un enregistrement de source de données non consolidé est lié à ou fusionné avec un autre enregistrement de données principal. Le processus de consolidation des données peut aboutir à la création d'un nouvel enregistrement de données principal ou à la liaison de l'enregistrement de données source à un enregistrement de données principal existant >> \fullcite[url: https://www.ibm.com/docs/fr/strategicsm/10.1.1?topic=master-data-mastering]{noauthor_ibm_2021}}. Le tableau de données principal \textit{df\_main} est assemblé par la consolidation de multiples tables d'enregistrements sources obtenues lors de la segmentation (\textit{df\_rentes}, \textit{df\_connetablies}, \textit{df\_escroetes}). Chaque que fois qu'un ensemble de données est requis, une partie du tableau de données \textit{df\_main} est copié pour créer un nouveau tableau ou vecteur de données. Cette stratégie de gestion des données  a été utilisée parce qu'elle est intuitive, aisée à mettre en place et, surtout, suffisante.

Cependant, cette gestion est quelque peu rigide :  les enregistrements sources doivent être mises à jours lorsque les données consolidées sont modifiées. Ceci implique la duplication d'un grand nombre de données et que si un enregistrement source comporte des anomalies, celle-ci seront propagées dans les nouveaux enregistrements et si plusieurs processus de consolidation se suivent,  la correction peuvent devenir vite laborieuse \parencite{boydens_qualite_2021}. Dans le cas où  les enregistrements sources ne sont pas mis à jour, comme ça l'est pour les tableaux  \textit{df\_rentes}, \textit{df\_connetablies} et \textit{df\_escroetes}, la mémoire du système s'encombrerait de nombreuses données obsolètes.

Un système de table de données \textit{SQL}  reliées par des clés étrangères permettrait serait une option plus lourde à mettre en place, mais qui permettrait une centralisation des données. 

Il s'agit ici d'une amélioration,à primineur, voire anodine, car dans le cas qui est le nôtre, le volume de données n'est pas assez conséquent pour que cela ait un impact notable. Toute foi, 

Il s'agit ici d'une amélioration, qui peut sembler mineur, voire anodine, car dans le cas qui est le nôtre, le volume de données n'est pas assez conséquent pour que cela ait un impact notable. Toute foi, si le système est repris à l'avenir et élargi afin de traiter plus de sources, cette négligence concernant la gestion des données pourrait engendrés des alourdissement du système.

