\section{Traitement de l'information}
%intro%
Les étapes de traitement de l'information reprennent toutes les opérations qui, d'une manière ou d'une autre, analysent et modifient le contenu originale du document source ou des informations extraites. Il peut s'agir d'audit de la qualité des informations contenues dans un tableau, de la mise en place de contraintes d'intégrité, de corrections de données ou encore de regroupements de données. 
L'objectif des opérations de traitement de l'information est d'améliorer la qualité des données extraites afin qu'elle soit utilisable dans les opérations d'exploitation des données décrites dans la section suivante.

\subsection{Analyse de la qualité des données}
% définition des critères de qualité
S'il n'existe pas de consensus sur la définition même de ce que représente la qualité des données, pour autant, tous s'accordent à dire que la qualité des données se décompose en une pluralité de critères dont la pertinence et le poids varient en fonction du cadre d'utilisation de ces informations \parencite{berti-equille_qualite_2004}. De ce constat est né le concept de \textit{fitness for use} qui dit que :
\begin{displayquote}
    \og La qualité de l'information ne renvoie jamais à la perfection de celle-ci mais à son adéquation relative à un ensemble de besoins donnés. La tolérance à l'erreur variera en fonction des enjeux.\fg{} 
    \footfullcite[p.7]{boydens_delivrable_2007} 
\end{displayquote} 
\vspace{0,5cm}

%enjeux de la qualité dans les process automatisés
Cette qualité des données est un aspect critique dans tout processus d'automatisation. C'est particulièrement vrai lorsque plusieurs opérations automatisées se succèdent sans vérification de l'agent humain. De petites erreurs en début de processus peuvent alors se propager et prendre de l'ampleur à travers chaque étapes suivantes. 

%exemple
Si, par exemple, suite à une erreur, une escroete n'est pas détectée par l'algorithme de segmentation et n'est, de facto,  pas non plus répertoriée. De cette seule erreur, découlerait qu'aucune connétablie située au sein de l'escroete ne serait alors répertoriée. Par extension, toutes les rentes situées dans ces connétablies seraient également non-indexées et omissent dans la suite du processus. Ainsi, une unique erreur peut finalement lourdement impacter les résultats obtenus en sortie du système.

%stratégie globale 
Afin d'éviter cela, des audits de la qualité des données récoltées ont été menés pendant la conception du système. En fonction des résultats rendus, les algorithmes ont été corrigés afin d'intégrer des contraintes d'intégrité (approche préventive) et des opérations de nettoyages des données(approche curative).
Dans le cas d'un système qui serait alimenté par des flux de données nouvelles, les phases d'audit devrait être intégrées au système lui-même ; on parlerait alors de \textit{gestion des processus} \parencite{berti-equille_qualite_2004}. Mais, dans le contexte qui est le nôtre, les données sont finies, plus aucunes nouvelles entrés ne sera ajoutées au registre de rentes, on peut donc sortir les phases d'audit du fonctionnement du système et les cantonner au développement du système.

\subsection{Nettoyage des données}
%définition 
Le nettoyage des données est un processus d'amélioration des données qui a pour but, au travers d'une série de transformations, de normaliser les données et de détecter lorsque plusieurs enregistrements font référence au même objet (phénomène de sur-couverture) \parencite{berti-equille_qualite_2004}.

%stratégie de nettoyage %
Les stratégies pour la correction des données erronées sont aussi diverses que les causes d'où ces erreurs peuvent provenir. C'est pour cela, qu'avant toute chose, il est impératif de comprendre la nature du document du source et des données qu'il contient. Il faut pouvoir supposer les différents facteurs qui auraient pu altérer l'information lors de son inscription dans le document source afin de mettre en place les vérifications nécessaire pour anticiper, et le cas échéant, corriger les erreurs contenus dans le document.

%Analyse du document et des données
Dans notre cas, si nous analysons \og l'histoire \fg{} du document source, nous pouvons dire qu'il s'agit d'un document texte au format \textit{.docx} qui  a été converti au format \textit{.txt}. Or, le format \textit{.docx} prend en charge des éléments d'édition de texte qui ne le sont pas dans les fichiers \textit{.txt}. On peut donc prévoir des erreurs liées à la perte d'informations engendrée par la conversion du ficher dans un format plus basique. 
Ce document \textit{.docx} est une retranscription manuelle d'une partie du livre de G.Espinas \textit{OCRisé}.
finalement, le livre de G. Espinas est lui même une retranscription du registre original de Sire Jean de France. Aux erreurs provenant du changement de format du document, nous pouvons donc aussi ajouter d'éventuelles erreurs de typographie ou d'OCR \parencite{berti-equille_qualite_2004}.
Quant aux données contenues dans le document, elles sont de nature textuelle en langue picarde du XIIIe siècle. Du fait de cette particularité, les outils de nettoyage classiques sont inefficients. A l'instar de la reconnaissance d'entités nommées, il faut dans notre cas retourner à une approche plus élémentaire, basée sur une reconnaissance morphologique à l'aide d'expressions régulières. 

%doublons%
Une autre source de la perte de qualité de l'information, serait la présence de doublon au sein des tableaux de données. On peut légitiment supposer qu'il n'y en a pas dans le registre original étant donné que celui-ci semble avoir été écrit de la main d'un seul homme  et que chacune des entrées du registre est classée selon un ordre topographique-administratif avec une identifiant unique \parencite{espinas_les_1933}. Cependant, la possibilité que des doublons soient produits par les opérations automatisées de segmentation et de traitement ne sont pas à négliger \parencite{koudoro-parfait_reconnaissance_2022}. Une vérification et une correction automatique sont donc effectuées après le remplissage de chaque tableau de données.

\subsection{Regroupement des données}
%définition
Le regroupement des données, appelé aussi \textit{clustering}, entre dans les stratégies courantes de la détection des doublons. Elle fait donc, par définition, partie des opérations de nettoyage des données. Comme sont nom l'indique, le regroupement de données va regrouper et fusionner les enregistrements d'un tableau de données qui partagent de fortes similitudes entres-eux et référençant un même objets. %add ref

% contexte

%methode calcule distance

%type de  clustering 



